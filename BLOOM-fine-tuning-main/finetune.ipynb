{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eDg5XB5XSOvy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pierre-Hadrien\\NanoGPT\\BLOOM-fine-tuning-main\\BLOOM-fine-tuning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'BLOOM-fine-tuning'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.27.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from -r requirements.txt (line 1)) (4.27.1)\n",
            "Requirement already satisfied: torch>=1.13.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from -r requirements.txt (line 2)) (1.13.1+cu116)\n",
            "Requirement already satisfied: datasets==2.10.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from -r requirements.txt (line 3)) (2.10.1)\n",
            "Requirement already satisfied: tqdm==4.65.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from -r requirements.txt (line 5)) (4.65.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: requests in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (0.13.2)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers==4.27.1->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (12.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: xxhash in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (2023.1.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from datasets==2.10.1->-r requirements.txt (line 3)) (0.18.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from torch>=1.13.1->-r requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 3)) (0.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from packaging>=20.0->transformers==4.27.1->-r requirements.txt (line 1)) (3.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from importlib-metadata->transformers==4.27.1->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pierre-hadrien\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.10.1->-r requirements.txt (line 3)) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/hyintell/BLOOM-fine-tuning.git\n",
        "%cd BLOOM-fine-tuning\n",
        "! pip install -r requirements.txt "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-fO-45SZ7r"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y1K0wM03SUC6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BloomTokenizerFast, BloomForCausalLM, TrainingArguments\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from utils import ModifiedTrainer, tokenise_data, data_collator\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BloomForCausalLM, BloomTokenizerFast\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v0pkIZelS8V6"
      },
      "source": [
        " # Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYJZvgDGS78C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XSVC2chxSTBS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 1557.61M\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bloom-560m\"\n",
        "use_gpt = True\n",
        "if use_gpt:\n",
        "    model = GPT.from_pretrained('gpt2-xl')\n",
        "else:\n",
        "    model = BloomForCausalLM.from_pretrained(f\"bigscience/{model_name}\")\n",
        "\n",
        "    \n",
        "tokeniser = BloomTokenizerFast.from_pretrained(f\"bigscience/{model_name}\", add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nqfIQ_EQTM3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset json (C:/Users/Pierre-Hadrien/.cache/huggingface/datasets/json/default-a88fadb3b7372eba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dd4655b82d14246a4562bf2c1e070bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 133.33it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 1600)\n",
              "    (wpe): Embedding(1024, 1600)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (24): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (25): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (26): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (27): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (28): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (29): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (30): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (31): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (32): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (33): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (34): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (35): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (36): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (37): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (38): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (39): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (40): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (41): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (42): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (43): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (44): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (45): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (46): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (47): Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
              "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
              "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
              "          (act): NewGELU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset = load_dataset('tatsu-lab/alpaca')\n",
        "dataset = load_dataset(\"json\", data_files=\"../nba_stats_with_text.json\")\n",
        "input_ids = tokenise_data(dataset, tokeniser)\n",
        "\n",
        "model.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from transformers import Trainer\n",
        "\n",
        "class ModifiedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs[\"input_ids\"].to(\"cpu\")\n",
        "        attention_mask = torch.ones_like(input_ids).bool().to(\"cpu\")\n",
        "        labels = inputs[\"input_ids\"].to(\"cpu\")\n",
        "        \n",
        "        if use_gpt:\n",
        "            return model(idx=input_ids).loss\n",
        "        else :\n",
        "            return model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            ).loss\n",
        "        \n",
        "\n",
        "\n",
        "def data_collator(features: list) -> dict:\n",
        "    return {\"input_ids\": torch.stack([torch.LongTensor(f) for f in features])}\n",
        "\n",
        "\n",
        "def tokenise_data(dataset, tokenizer, max_seq_length=512):\n",
        "    tokenised_list = []\n",
        "    for elem in tqdm.tqdm(dataset[\"train\"]):\n",
        "        tokenised_list.append(\n",
        "            tokenizer.encode(\n",
        "                elem[\"text\"],\n",
        "                max_length=max_seq_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "            )\n",
        "        )\n",
        "    return tokenised_list\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(default=\"bigscience/bloom-560m\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_name_or_path: str = field(\n",
        "        default=\"tatsu-lab/alpaca\", metadata={\"help\": \"Path to the training data.\"}\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "j02eVqAoTWsU"
      },
      "outputs": [],
      "source": [
        "model.is_parallelizable = True\n",
        "model.model_parallel = True\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"output\",\n",
        "    fp16=False,\n",
        "    gradient_accumulation_steps= 1,\n",
        "    per_device_train_batch_size = 2,\n",
        "    learning_rate = 2e-5,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = ModifiedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=input_ids,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Pierre-Hadrien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f94a8924ac54236949374ac621ecccd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10084\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1638\u001b[0m         )\n\u001b[0;32m   1639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                 if (\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2644\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2645\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10084\\2109914798.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_gpt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             return model(\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Pierre-Hadrien\\NanoGPT\\BLOOM-fine-tuning-main\\mingpt\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;31m# forward the GPT model itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mtok_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# token embeddings of shape (b, t, n_embd)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# position embeddings of shape (1, t, n_embd)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    160\u001b[0m         return F.embedding(\n\u001b[0;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "    # Load the trained model\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input prompt into integer input sequence\n",
        "    encoded_input = tokeniser(prompt, return_tensors='pt').to(device)\n",
        "    x = encoded_input['input_ids']\n",
        "\n",
        "    # We'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # Forward the model `steps` times to get samples, in a batch\n",
        "    with torch.no_grad():\n",
        "        y = model.generate(\n",
        "            x,\n",
        "            max_length= 100,  # Set max_length based on input sequence length + steps\n",
        "            do_sample=do_sample,\n",
        "            top_k=40,\n",
        "            pad_token_id=tokeniser.pad_token_id\n",
        "        )\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokeniser.decode(y[i].cpu().squeeze().tolist())\n",
        "        print('-' * 80)\n",
        "        print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Herbert Jones, game:\n",
            "player: Herbert Jones, player: player, stats: [player: player, player: player, player: player, player: player, player: player], score: 0, game: game, player: player, player: player.]\n",
            "I think the player stats should now include the player, game, player, player, player, player, player, player, player, player, player, player, player, player\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Herbert Jones\n",
            "\n",
            "This code will save the player stats of player Herbert Jones, who is found in the player statistics of player Herbert Jones, and is also the player in the player of player Herbert Jones player statistics, and player and player in player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player player\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Herbert Jones, who has played 18 games, averaged 4.3 minutes per game, and tackled 1.1 per game. Herbert Jones has an average of 4.2 personal fouls, which is 5 percent of the field goal rate. In a field goal percentage of 20.1, Herbert Jones ranks 23rd. In a field goal percentage of 18.8, he ranks 21st. In a field goal percentage of 18.4, he ranks 26th\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Herbert Jones for game: 12 games. He has an average of 5 points, 2.6 rebounds, 0.7 assists, and 0.8 steals during his collegiate career with the University of Michigan. He also made 10 three-pointers during the game. He also ranks 12th in the national average after making 18 threepointers, he made a double three-point game and made 6 rebounds. He also made 7 three-pointers and made\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Herbert Jones, player: Cain Lewis\n",
            "\n",
            "How to:\n",
            "\n",
            "Create a player stats file for player Herbert Jones, player: Cain Lewis\n",
            "\n",
            "NOTE:\n",
            "Players must have at least two of their first names and last names\n",
            "\n",
            "Examples:\n",
            "\n",
            "Creating player stats file for player: Herbert Jones, player: Cain Lewis\n",
            "\n",
            "NOTE:\n",
            "Players must have two of their first names, last names, and field ids and also have two player stats (max. with fieldIds\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Generate specific player stats for player: Herbert Jones', num_samples=5, steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Carlik Jones\n",
            "    *    If player has a player statistics, player statistics are saved for the player.\n",
            "    *\n",
            "    * @param player The player data to store player stats\n",
            "    */\n",
            "    void setPlayerStats(Creature & player);\n",
            "\n",
            "    /**\n",
            "    * Changes player stats for player: Rengar Targaryen\n",
            "    *    If player has a player statistics, player stats are saved for the player.\n",
            "    *\n",
            "    * @param player The player data to store player stats\n",
            "   \n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Carlik Jones has an average of 0.5 possessions, 0 attempts, 0 steals, 0 field goals, and 0 total field goals for he has a field goal percentage of 1.6%. The field goal percentage for Carlik Jones is 0.6% and he makes 0 field goal attempts per game. Carlik Jones makes 0 three-pointers and 0 three-pointers for the entire game for the Coyotes. Carlik Jones has a personal fouls\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Carlik Jones.\", player, player.id);\n",
            "\n",
            "        for (let player in player.players)\n",
            "        {\n",
            "            player.stats.value.push({\n",
            "                label: player.options.player_title.name,\n",
            "                value: player.players.indexOf(player).maxValue\n",
            "            });\n",
            "        }\n",
            "        player.stats.value.forEach(player => player.stats.value.push({\"player\": player.id, \"value\": player.options.\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Carlik Jones\n",
            "-- Adds player statistics for player: Carlik Jones\n",
            "carlik_jn = Player({\n",
            "    name = Carlik Jones,\n",
            "    rank = 0,\n",
            "    num_likes = 0,\n",
            "    num_ranks = 3,\n",
            "    stats_id = 23,\n",
            "    talents_id = 25,\n",
            "    stats_flags = 0,\n",
            "    stats_data = {\n",
            "        stats_rating = 0,\n",
            "        stats_age = 0,\n",
            "        stats_weight = 0,\n",
            "        stats_healing = 0,\n",
            "        stats_rating\n",
            "--------------------------------------------------------------------------------\n",
            "Generate specific player stats for player: Carlik Jones is a player of the Lichfield Blues. The average value for player: Carlik Jones is 0. The average is 0 points. player: Carlic Jones is one of the players of the Lichfield Blues. player:A player of the Lichfield Blues is one of the players. player:A player of the Lichfield Blues is played as an average player. player:A player of the Lichfield Blues is an average player.\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Generate specific player stats for player: Carlik Jones', num_samples=5, steps=30)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
