{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDg5XB5XSOvy"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/hyintell/BLOOM-fine-tuning.git\n",
        "%cd BLOOM-fine-tuning\n",
        "! pip install -r requirements.txt "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-fO-45SZ7r"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y1K0wM03SUC6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BloomTokenizerFast, BloomForCausalLM, TrainingArguments\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from utils import ModifiedTrainer, tokenise_data, data_collator"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v0pkIZelS8V6"
      },
      "source": [
        " # Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rYJZvgDGS78C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XSVC2chxSTBS"
      },
      "outputs": [],
      "source": [
        "model_name = \"bloom-560m\"\n",
        "model = BloomForCausalLM.from_pretrained(f\"bigscience/{model_name}\")\n",
        "tokeniser = BloomTokenizerFast.from_pretrained(f\"bigscience/{model_name}\", add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nqfIQ_EQTM3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default to C:/Users/Pierre-Hadrien/.cache/huggingface/datasets/json/default-a88fadb3b7372eba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3110e9d8a9e4443b85982710ea9e22bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4e86c6c6f014fefa06e076e83d9dea7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c89bbd3814c9420bb80d54d15a4425f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to C:/Users/Pierre-Hadrien/.cache/huggingface/datasets/json/default-a88fadb3b7372eba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5668b04614e4ae789f386b26cd43178",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 357.16it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BloomForCausalLM(\n",
              "  (transformer): BloomModel(\n",
              "    (word_embeddings): Embedding(250880, 1024)\n",
              "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (h): ModuleList(\n",
              "      (0): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (2): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (3): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (4): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (5): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (6): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (7): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (8): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (9): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (10): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (11): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (12): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (13): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (14): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (15): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (16): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (17): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (18): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (19): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (20): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (21): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (22): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (23): BloomBlock(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): BloomAttention(\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): BloomMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (gelu_impl): BloomGelu()\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset = load_dataset('tatsu-lab/alpaca')\n",
        "dataset = load_dataset(\"json\", data_files=\"../nba_stats_with_text.json\")\n",
        "input_ids = tokenise_data(dataset, tokeniser)\n",
        "\n",
        "model.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from transformers import Trainer\n",
        "\n",
        "class ModifiedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = torch.ones_like(input_ids).bool().to(model.device)\n",
        "        labels = inputs[\"input_ids\"].to(model.device)\n",
        "        \n",
        "        return model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        ).loss\n",
        "\n",
        "\n",
        "def data_collator(features: list) -> dict:\n",
        "    return {\"input_ids\": torch.stack([torch.LongTensor(f) for f in features])}\n",
        "\n",
        "\n",
        "def tokenise_data(dataset, tokenizer, max_seq_length=512):\n",
        "    tokenised_list = []\n",
        "    for elem in tqdm.tqdm(dataset[\"train\"]):\n",
        "        tokenised_list.append(\n",
        "            tokenizer.encode(\n",
        "                elem[\"text\"],\n",
        "                max_length=max_seq_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "            )\n",
        "        )\n",
        "    return tokenised_list\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(default=\"bigscience/bloom-560m\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_name_or_path: str = field(\n",
        "        default=\"tatsu-lab/alpaca\", metadata={\"help\": \"Path to the training data.\"}\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j02eVqAoTWsU"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model.is_parallelizable = True\n",
        "model.model_parallel = True\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"output\",\n",
        "    fp16=False,\n",
        "    gradient_accumulation_steps= 1,\n",
        "    per_device_train_batch_size = 2,\n",
        "    learning_rate = 2e-5,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = ModifiedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=input_ids,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Pierre-Hadrien\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19dbb1cde6de4adea417631d2eb74794",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 8.2109, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "{'train_runtime': 936.4405, 'train_samples_per_second': 0.021, 'train_steps_per_second': 0.011, 'train_loss': 8.210888671875, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=8.210888671875, metrics={'train_runtime': 936.4405, 'train_samples_per_second': 0.021, 'train_steps_per_second': 0.011, 'train_loss': 8.210888671875, 'epoch': 2.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "    # Load the trained model\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input prompt into integer input sequence\n",
        "    encoded_input = tokeniser(prompt, return_tensors='pt').to(device)\n",
        "    x = encoded_input['input_ids']\n",
        "\n",
        "    # We'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # Forward the model `steps` times to get samples, in a batch\n",
        "    with torch.no_grad():\n",
        "        y = model.generate(\n",
        "            x,\n",
        "            max_length= 100,  # Set max_length based on input sequence length + steps\n",
        "            do_sample=do_sample,\n",
        "            top_k=40,\n",
        "            pad_token_id=tokeniser.pad_token_id\n",
        "        )\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokeniser.decode(y[i].cpu().squeeze().tolist())\n",
        "        print('-' * 80)\n",
        "        print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\"Generate specific player stats for player: Herbert Jones.\", \"race\", { }, true, player, \"hernandez\", { }, { }, {}, {}, { }, { }, { }, { }, { }, { }, { }, { }, { \", \", \", \", \"}\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, \", \", \", \", \"}\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, \", \", \", \", \"}\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, \", \", \", \", \"}\", 0, 0, 0, 0, 0, 0, 0, 0,\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='\"Generate specific player stats for player: Herbert Jones', num_samples=5, steps=30)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
